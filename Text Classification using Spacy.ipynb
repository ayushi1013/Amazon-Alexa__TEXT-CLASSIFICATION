{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"Text Classification using Spacy.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-05-10T04:10:17.767697Z","start_time":"2021-05-10T04:10:11.836803Z"},"id":"mJwKhPLki0WD","executionInfo":{"status":"ok","timestamp":1620705001042,"user_tz":-330,"elapsed":5266,"user":{"displayName":"Rishabh Fafriya","photoUrl":"","userId":"12081523781590629366"}}},"source":["import pandas as pd\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","from sklearn.base import TransformerMixin\n","from sklearn.pipeline import Pipeline"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-05-10T04:10:54.542273Z","start_time":"2021-05-10T04:10:54.349723Z"},"colab":{"base_uri":"https://localhost:8080/","height":446},"id":"3Q6NkQo8i0WM","executionInfo":{"status":"error","timestamp":1620705007326,"user_tz":-330,"elapsed":2269,"user":{"displayName":"Rishabh Fafriya","photoUrl":"","userId":"12081523781590629366"}},"outputId":"6eca9ef8-934d-4079-fb3f-c8c8d7273efe"},"source":["# Loading TSV file\n","df_amazon = pd.read_csv (\"amazon_alexa.tsv\", sep=\"\\t\")\n","\n","# Top 5 records\n","df_amazon.head()"],"execution_count":2,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-e5e3bd02c1e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading TSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_amazon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"amazon_alexa.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Top 5 records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_amazon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'amazon_alexa.tsv'"]}]},{"cell_type":"markdown","metadata":{"id":"K-kbkl4LjGOu"},"source":[""]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-05-10T04:14:47.063151Z","start_time":"2021-05-10T04:14:47.051944Z"},"id":"Y0kOlvoCi0WO","outputId":"926cc049-14d8-4226-accc-9c294563b3e8"},"source":["df_amazon.feedback.value_counts(normalize=True)*100"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    91.84127\n","0     8.15873\n","Name: feedback, dtype: float64"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-05-10T04:16:38.709117Z","start_time":"2021-05-10T04:16:38.687922Z"},"id":"geLEHetui0WP","outputId":"d6c1e70a-b997-4be1-fd84-505d5559ac31"},"source":["df_amazon.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 3150 entries, 0 to 3149\n","Data columns (total 5 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   rating            3150 non-null   int64 \n"," 1   date              3150 non-null   object\n"," 2   variation         3150 non-null   object\n"," 3   verified_reviews  3150 non-null   object\n"," 4   feedback          3150 non-null   int64 \n","dtypes: int64(2), object(3)\n","memory usage: 123.2+ KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-05-10T04:19:25.362534Z","start_time":"2021-05-10T04:19:25.281920Z"},"id":"3jF3TKKQi0WP"},"source":["import string\n","from spacy.lang.en.stop_words import STOP_WORDS\n","from spacy.lang.en import English\n","\n","# Create our list of punctuation marks\n","punctuations = string.punctuation\n","\n","# stopwords\n","stop_words = STOP_WORDS\n","\n","# Load English tokenizer, tagger, parser, NER and word vectors\n","nlp = English()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-05-10T04:21:27.295144Z","start_time":"2021-05-10T04:21:27.288393Z"},"id":"ZMvC65Ovi0WQ"},"source":["# Creating our tokenizer function\n","def spacy_tokenizer(sentence):\n","    # Creating our token object, which is used to create documents with linguistic annotations.\n","    mytokens = nlp(sentence)\n","    \n","    # Lemmatizing each token and converting each token into lowercase\n","    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens]\n","    \n","    # Removing stop words\n","    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n","\n","    # return preprocessed list of tokens\n","    return mytokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-05-10T04:23:52.600875Z","start_time":"2021-05-10T04:23:52.592094Z"},"id":"wQLpKth9i0WQ"},"source":["class predictors(TransformerMixin):\n","    \n","    def transform(self, X, **transform_params):\n","        # Cleaning Text\n","        return [clean_text(text) for text in X]\n","\n","    def fit(self, X, y=None, **fit_params):\n","        return self\n","    \n","    def get_params(self, deep=True):\n","        return {}\n","\n","# Basic function to clean the text\n","def clean_text(text):\n","    # Removing spaces and converting text into lowercase\n","    return text.strip().lower()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-05-10T04:25:23.299478Z","start_time":"2021-05-10T04:25:23.284398Z"},"id":"sl0gt08vi0WR"},"source":["# Bag of words vectorization\n","bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n","\n","# Tf-IDF vectorizationj\n","tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-05-10T04:24:24.181871Z","start_time":"2021-05-10T04:24:23.863100Z"},"id":"4aYv7F6gi0WR"},"source":["from sklearn.model_selection import train_test_split\n","\n","X = df_amazon['verified_reviews'] # the features we want to analyze\n","ylabels = df_amazon['feedback'] \n","\n","# the labels, or answers, we want to test against\n","X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-05-10T04:46:58.845750Z","start_time":"2021-05-10T04:46:57.963054Z"},"id":"_gopey7Ti0WR","outputId":"8d572664-08ca-4c65-db7c-916031f0837e"},"source":["# Logistic Regression Classifier\n","from sklearn.linear_model import LogisticRegression\n","classifier = LogisticRegression()\n","\n","# Create pipeline using Bag of Words\n","pipe = Pipeline([('vectorizer', tfidf_vector),\n","                 ('classifier', classifier)])\n","\n","# model generation\n","pipe.fit(X_train,y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(steps=[('vectorizer',\n","                 TfidfVectorizer(tokenizer=<function spacy_tokenizer at 0x7f2d260b7280>)),\n","                ('classifier', LogisticRegression())])"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-05-10T04:47:44.530041Z","start_time":"2021-05-10T04:47:44.161470Z"},"id":"Yg1mxJx9i0WS","outputId":"182a203e-4a66-4eda-bb4c-6fb2ef1d6332"},"source":["from sklearn import metrics\n","\n","# Predicting with a test dataset\n","predicted = pipe.predict(X_test)\n","\n","# Model Accuracy\n","print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n","print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n","print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Logistic Regression Accuracy: 0.9185185185185185\n","Logistic Regression Precision: 0.9184322033898306\n","Logistic Regression Recall: 1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hWRxAHp9i0WS"},"source":[""],"execution_count":null,"outputs":[]}]}